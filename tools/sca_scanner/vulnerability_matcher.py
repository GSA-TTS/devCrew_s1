"""
Vulnerability Matcher Module

Queries vulnerability databases (NVD, OSV, GitHub Advisory) to match package
versions against known CVEs with CVSS scoring and local caching support.
"""

import hashlib
import json
import logging
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

import requests
from cachetools import TTLCache
from packaging.specifiers import SpecifierSet
from packaging.version import InvalidVersion, parse

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


class VulnerabilityMatcherError(Exception):
    """Base exception for VulnerabilityMatcher errors."""

    pass


class RateLimitError(VulnerabilityMatcherError):
    """Raised when API rate limit is exceeded."""

    pass


class APIError(VulnerabilityMatcherError):
    """Raised when API returns an error."""

    pass


class VulnerabilityMatcher:
    """
    Vulnerability database matcher with NVD, OSV, GitHub Advisory support.

    Implements caching, rate limiting, and batch processing for
    efficient vulnerability lookups across multiple databases.
    """

    # API endpoints
    NVD_API_URL = "https://services.nvd.nist.gov/rest/json/cves/2.0"
    OSV_API_URL = "https://api.osv.dev/v1"
    GITHUB_API_URL = "https://api.github.com/graphql"

    # CVSS severity thresholds (CVSS 3.1)
    SEVERITY_THRESHOLDS = {
        "CRITICAL": (9.0, 10.0),
        "HIGH": (7.0, 8.9),
        "MEDIUM": (4.0, 6.9),
        "LOW": (0.1, 3.9),
        "NONE": (0.0, 0.0),
    }

    # Ecosystem mappings for different databases
    ECOSYSTEM_MAP = {
        "python": {"osv": "PyPI", "github": "PIP"},
        "npm": {"osv": "npm", "github": "NPM"},
        "maven": {"osv": "Maven", "github": "MAVEN"},
        "go": {"osv": "Go", "github": "GO"},
        "ruby": {"osv": "RubyGems", "github": "RUBYGEMS"},
        "rust": {"osv": "crates.io", "github": "RUST"},
    }

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize VulnerabilityMatcher.

        Args:
            config: Configuration dictionary with optional keys:
                - cache_ttl: Cache time-to-live in seconds (default: 86400)
                - databases: List of databases to query (default: all)
                - api_keys: Dictionary of API keys for databases
                - rate_limit: Rate limiting configuration
                - cache_dir: Directory for persistent cache
        """
        self.config = config or {}

        # Cache configuration
        cache_ttl = self.config.get("cache_ttl", 86400)
        self._memory_cache: TTLCache = TTLCache(maxsize=1000, ttl=cache_ttl)
        self._cache_dir = Path(self.config.get("cache_dir", ".sca_cache"))
        self._cache_dir.mkdir(exist_ok=True)

        # Database selection
        self.databases = self.config.get("databases", ["osv", "nvd", "github"])

        # API keys
        api_keys = self.config.get("api_keys", {})
        self.nvd_api_key = api_keys.get("nvd")
        self.github_token = api_keys.get("github")

        # Rate limiting
        rate_config = self.config.get("rate_limit", {})
        self.requests_per_minute = rate_config.get("requests_per_minute", 30)
        self._request_times: List[float] = []

        # Session for connection pooling
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "SCA-Scanner/1.0",
                "Accept": "application/json",
            }
        )

        if self.github_token:
            self.session.headers.update({"Authorization": f"token {self.github_token}"})

        logger.info(
            f"VulnerabilityMatcher initialized with " f"databases: {self.databases}"
        )

    def find_vulnerabilities(
        self, dependencies: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Find vulnerabilities for a list of dependencies.

        Args:
            dependencies: List of dependency dictionaries with keys:
                - package: Package name
                - version: Package version
                - ecosystem: Package ecosystem (python, npm, etc.)

        Returns:
            List of vulnerability dictionaries
        """
        all_vulnerabilities = []
        total = len(dependencies)

        logger.info(f"Scanning {total} dependencies for vulnerabilities")

        for idx, dep in enumerate(dependencies, 1):
            package = dep.get("package")
            version = dep.get("version")
            ecosystem = dep.get("ecosystem")

            if not all([package, version, ecosystem]):
                logger.warning(f"Skipping incomplete dependency: {dep}")
                continue

            # Type narrowing - we know these are strings now
            assert isinstance(package, str)
            assert isinstance(version, str)
            assert isinstance(ecosystem, str)

            logger.info(f"[{idx}/{total}] Checking {ecosystem}:{package}@{version}")

            try:
                vulns = self._find_package_vulnerabilities(package, version, ecosystem)
                if vulns:
                    logger.info(f"  Found {len(vulns)} vulnerabilities")
                    all_vulnerabilities.extend(vulns)
                else:
                    logger.debug("No vulnerabilities found")
            except Exception as e:
                logger.error(f"Error scanning {package}@{version}: {str(e)}")

        logger.info(
            f"Scan complete: {len(all_vulnerabilities)} " f"total vulnerabilities found"
        )
        return all_vulnerabilities

    def _find_package_vulnerabilities(
        self, package: str, version: str, ecosystem: str
    ) -> List[Dict[str, Any]]:
        """
        Find vulnerabilities for a single package.

        Args:
            package: Package name
            version: Package version
            ecosystem: Package ecosystem

        Returns:
            List of vulnerability dictionaries
        """
        # Check cache first
        cache_key = self._get_cache_key(package, version, ecosystem)
        cached = self._get_from_cache(cache_key)
        if cached is not None:
            logger.debug(f"Cache hit for {package}@{version}")
            return cached

        vulnerabilities = []

        # Query each enabled database
        for db in self.databases:
            try:
                if db == "osv":
                    vulns = self.query_osv(package, version, ecosystem)
                elif db == "nvd":
                    vulns = self.query_nvd(package, version, ecosystem)
                elif db == "github":
                    vulns = self.query_github(package, version, ecosystem)
                else:
                    logger.warning(f"Unknown database: {db}")
                    continue

                vulnerabilities.extend(vulns)
            except RateLimitError as e:
                logger.warning(f"Rate limit hit for {db}: {str(e)}")
                # Continue with other databases
            except APIError as e:
                logger.error(f"API error for {db}: {str(e)}")
            except Exception as e:
                logger.error(f"Unexpected error querying {db}: {str(e)}")

        # Deduplicate by CVE ID
        vulnerabilities = self._deduplicate_vulnerabilities(vulnerabilities)

        # Cache results
        self._save_to_cache(cache_key, vulnerabilities)

        return vulnerabilities

    def query_osv(
        self, package: str, version: str, ecosystem: str
    ) -> List[Dict[str, Any]]:
        """
        Query OSV (Open Source Vulnerabilities) database.

        Args:
            package: Package name
            version: Package version
            ecosystem: Package ecosystem

        Returns:
            List of vulnerability dictionaries
        """
        self._check_rate_limit()

        osv_ecosystem = self.ECOSYSTEM_MAP.get(ecosystem, {}).get(
            "osv", ecosystem.upper()
        )

        payload = {
            "package": {"name": package, "ecosystem": osv_ecosystem},
            "version": version,
        }

        try:
            response = self.session.post(
                f"{self.OSV_API_URL}/query",
                json=payload,
                timeout=30,
            )
            response.raise_for_status()
            data = response.json()
        except requests.exceptions.RequestException as e:
            raise APIError(f"OSV API request failed: {str(e)}")

        vulnerabilities = []
        for vuln_data in data.get("vulns", []):
            vuln = self._parse_osv_vulnerability(vuln_data, package, version, ecosystem)
            if vuln:
                vulnerabilities.append(vuln)

        return vulnerabilities

    def query_nvd(
        self, package: str, version: str, ecosystem: str
    ) -> List[Dict[str, Any]]:
        """
        Query NVD (National Vulnerability Database).

        Args:
            package: Package name
            version: Package version
            ecosystem: Package ecosystem

        Returns:
            List of vulnerability dictionaries
        """
        self._check_rate_limit()

        # Build search keyword
        keyword = f"{ecosystem} {package}"

        params: Dict[str, Any] = {
            "keywordSearch": keyword,
            "resultsPerPage": 100,
        }

        headers = {}
        if self.nvd_api_key:
            headers["apiKey"] = self.nvd_api_key

        try:
            response = self.session.get(
                self.NVD_API_URL,
                params=params,
                headers=headers,
                timeout=30,
            )

            if response.status_code == 403:
                raise RateLimitError("NVD API rate limit exceeded")

            response.raise_for_status()
            data = response.json()
        except requests.exceptions.RequestException as e:
            raise APIError(f"NVD API request failed: {str(e)}")

        vulnerabilities = []
        for vuln_item in data.get("vulnerabilities", []):
            cve_data = vuln_item.get("cve", {})
            vuln = self._parse_nvd_vulnerability(cve_data, package, version, ecosystem)
            if vuln and self._is_version_affected(version, vuln):
                vulnerabilities.append(vuln)

        return vulnerabilities

    def query_github(
        self, package: str, version: str, ecosystem: str
    ) -> List[Dict[str, Any]]:
        """
        Query GitHub Security Advisory Database.

        Args:
            package: Package name
            version: Package version
            ecosystem: Package ecosystem

        Returns:
            List of vulnerability dictionaries
        """
        if not self.github_token:
            logger.debug("GitHub token not provided, skipping GitHub database")
            return []

        self._check_rate_limit()

        github_ecosystem = self.ECOSYSTEM_MAP.get(ecosystem, {}).get(
            "github", ecosystem.upper()
        )

        query = """
        query($ecosystem: SecurityAdvisoryEcosystem!, $package: String!) {
          securityVulnerabilities(
            first: 100
            ecosystem: $ecosystem
            package: $package
          ) {
            nodes {
              advisory {
                ghsaId
                summary
                description
                severity
                publishedAt
                references {
                  url
                }
                identifiers {
                  type
                  value
                }
              }
              vulnerableVersionRange
              firstPatchedVersion {
                identifier
              }
            }
          }
        }
        """

        variables = {
            "ecosystem": github_ecosystem,
            "package": package,
        }

        try:
            response = self.session.post(
                self.GITHUB_API_URL,
                json={"query": query, "variables": variables},
                timeout=30,
            )

            if response.status_code == 403:
                raise RateLimitError("GitHub API rate limit exceeded")

            response.raise_for_status()
            data = response.json()
        except requests.exceptions.RequestException as e:
            raise APIError(f"GitHub API request failed: {str(e)}")

        if "errors" in data:
            logger.error(f"GitHub GraphQL errors: {data['errors']}")
            return []

        vulnerabilities = []
        nodes = data.get("data", {}).get("securityVulnerabilities", {}).get("nodes", [])

        for node in nodes:
            vuln = self._parse_github_vulnerability(node, package, version, ecosystem)
            if vuln and self._is_version_in_range(
                version, node.get("vulnerableVersionRange", "")
            ):
                vulnerabilities.append(vuln)

        return vulnerabilities

    def _parse_osv_vulnerability(
        self,
        vuln_data: Dict[str, Any],
        package: str,
        version: str,
        ecosystem: str,
    ) -> Optional[Dict[str, Any]]:
        """Parse OSV vulnerability data into standard format."""
        vuln_id = vuln_data.get("id", "")

        # Extract CVE ID if present
        cve_id = None
        for alias in vuln_data.get("aliases", []):
            if alias.startswith("CVE-"):
                cve_id = alias
                break

        if not cve_id:
            cve_id = vuln_id

        # Extract CVSS score
        cvss_score = 0.0
        cvss_vector = None
        for severity in vuln_data.get("severity", []):
            if severity.get("type") == "CVSS_V3":
                cvss_vector = severity.get("score", "")
                # Parse CVSS score from vector
                cvss_score = self._parse_cvss_vector(cvss_vector)
                break

        # Get fixed version
        fixed_version = None
        for affected in vuln_data.get("affected", []):
            if affected.get("package", {}).get("name") == package:
                for range_data in affected.get("ranges", []):
                    for event in range_data.get("events", []):
                        if "fixed" in event:
                            fixed_version = event["fixed"]
                            break

        # Extract description
        description = vuln_data.get("summary", "")
        if not description:
            description = vuln_data.get("details", "")

        # Extract references
        references = [
            ref.get("url") for ref in vuln_data.get("references", []) if ref.get("url")
        ]

        # Get published date
        published = vuln_data.get("published", "")
        published_date = published.split("T")[0] if published else ""

        return {
            "cve": cve_id,
            "package": package,
            "version": version,
            "severity": self.get_severity(cvss_score),
            "cvss_score": cvss_score,
            "cvss_vector": cvss_vector,
            "description": description[:500],
            "references": references,
            "fixed_in": fixed_version,
            "published_date": published_date,
            "database": "osv",
        }

    def _parse_nvd_vulnerability(
        self,
        cve_data: Dict[str, Any],
        package: str,
        version: str,
        ecosystem: str,
    ) -> Optional[Dict[str, Any]]:
        """Parse NVD vulnerability data into standard format."""
        cve_id = cve_data.get("id", "")

        # Extract CVSS 3.1 metrics
        cvss_score = 0.0
        cvss_vector = None

        metrics = cve_data.get("metrics", {})
        cvss_v31 = metrics.get("cvssMetricV31", [])
        if cvss_v31:
            cvss_data = cvss_v31[0].get("cvssData", {})
            cvss_score = cvss_data.get("baseScore", 0.0)
            cvss_vector = cvss_data.get("vectorString", "")
        else:
            # Fallback to CVSS 3.0
            cvss_v30 = metrics.get("cvssMetricV30", [])
            if cvss_v30:
                cvss_data = cvss_v30[0].get("cvssData", {})
                cvss_score = cvss_data.get("baseScore", 0.0)
                cvss_vector = cvss_data.get("vectorString", "")

        # Extract description
        descriptions = cve_data.get("descriptions", [])
        description = ""
        for desc in descriptions:
            if desc.get("lang") == "en":
                description = desc.get("value", "")
                break

        # Extract references
        references = [
            ref.get("url") for ref in cve_data.get("references", []) if ref.get("url")
        ]

        # Get published date
        published = cve_data.get("published", "")
        published_date = published.split("T")[0] if published else ""

        return {
            "cve": cve_id,
            "package": package,
            "version": version,
            "severity": self.get_severity(cvss_score),
            "cvss_score": cvss_score,
            "cvss_vector": cvss_vector,
            "description": description[:500],
            "references": references[:5],
            "fixed_in": None,
            "published_date": published_date,
            "database": "nvd",
        }

    def _parse_github_vulnerability(
        self,
        node: Dict[str, Any],
        package: str,
        version: str,
        ecosystem: str,
    ) -> Optional[Dict[str, Any]]:
        """Parse GitHub Advisory data into standard format."""
        advisory = node.get("advisory", {})

        # Extract CVE ID
        cve_id = advisory.get("ghsaId", "")
        for identifier in advisory.get("identifiers", []):
            if identifier.get("type") == "CVE":
                cve_id = identifier.get("value", cve_id)
                break

        # Map severity to CVSS score approximation
        severity_map = {
            "CRITICAL": 9.5,
            "HIGH": 7.5,
            "MODERATE": 5.5,
            "LOW": 2.5,
        }
        severity_str = advisory.get("severity", "MODERATE")
        cvss_score = severity_map.get(severity_str, 5.0)

        # Extract fixed version
        fixed_version = None
        first_patched = node.get("firstPatchedVersion")
        if first_patched:
            fixed_version = first_patched.get("identifier")

        # Extract references
        references = [
            ref.get("url") for ref in advisory.get("references", []) if ref.get("url")
        ]

        # Get published date
        published = advisory.get("publishedAt", "")
        published_date = published.split("T")[0] if published else ""

        description = advisory.get("summary", "")
        if not description:
            description = advisory.get("description", "")

        return {
            "cve": cve_id,
            "package": package,
            "version": version,
            "severity": severity_str,
            "cvss_score": cvss_score,
            "cvss_vector": None,
            "description": description[:500],
            "references": references[:5],
            "fixed_in": fixed_version,
            "published_date": published_date,
            "database": "github",
        }

    def get_severity(self, cvss_score: float) -> str:
        """
        Convert CVSS score to severity level.

        Args:
            cvss_score: CVSS 3.1 base score (0.0-10.0)

        Returns:
            Severity level: CRITICAL, HIGH, MEDIUM, LOW, or NONE
        """
        for severity, (low, high) in self.SEVERITY_THRESHOLDS.items():
            if low <= cvss_score <= high:
                return severity
        return "UNKNOWN"

    def _parse_cvss_vector(self, vector: str) -> float:
        """
        Parse CVSS score from vector string.

        Args:
            vector: CVSS vector string

        Returns:
            Base score as float
        """
        # Simple extraction - assumes format like "CVSS:3.1/AV:N/..."
        # In production, use a proper CVSS library
        if not vector:
            return 0.0

        # For now, return a default score
        # TODO: Implement proper CVSS vector parsing
        return 5.0

    def _is_version_affected(self, version: str, vuln: Dict[str, Any]) -> bool:
        """
        Check if a version is affected by a vulnerability.

        Args:
            version: Package version to check
            vuln: Vulnerability dictionary

        Returns:
            True if version is affected
        """
        # Simplified version checking
        # In production, implement proper version range comparison
        return True

    def _is_version_in_range(self, version: str, range_str: str) -> bool:
        """
        Check if version falls within a vulnerability range.

        Args:
            version: Package version
            range_str: Version range string (e.g., ">= 1.0.0, < 2.0.0")

        Returns:
            True if version is in range
        """
        if not range_str:
            return True

        try:
            pkg_version = parse(version)
            # Parse range format like ">= 1.0.0, < 2.0.0"
            range_str = range_str.replace(" ", "")
            spec = SpecifierSet(range_str)
            return pkg_version in spec
        except (InvalidVersion, ValueError) as e:
            logger.debug(f"Version parsing error: {str(e)}")
            return True

    def _deduplicate_vulnerabilities(
        self, vulnerabilities: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Remove duplicate vulnerabilities based on CVE ID.

        Args:
            vulnerabilities: List of vulnerability dictionaries

        Returns:
            Deduplicated list
        """
        seen: Set[str] = set()
        unique = []

        for vuln in vulnerabilities:
            cve_id = vuln.get("cve", "")
            if cve_id and cve_id not in seen:
                seen.add(cve_id)
                unique.append(vuln)
            elif not cve_id:
                # Keep vulnerabilities without CVE ID
                unique.append(vuln)

        return unique

    def _get_cache_key(self, package: str, version: str, ecosystem: str) -> str:
        """Generate cache key for a package."""
        key_str = f"{ecosystem}:{package}:{version}"
        return hashlib.sha256(key_str.encode()).hexdigest()

    def _get_from_cache(self, cache_key: str) -> Optional[List[Dict[str, Any]]]:
        """
        Retrieve cached results.

        Args:
            cache_key: Cache key

        Returns:
            Cached vulnerabilities or None
        """
        # Check memory cache first
        if cache_key in self._memory_cache:
            return self._memory_cache[cache_key]

        # Check disk cache
        cache_file = self._cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, "r") as f:
                    cache_data = json.load(f)

                # Check if cache is still valid
                cached_time = datetime.fromisoformat(cache_data.get("timestamp", ""))
                cache_ttl = self.config.get("cache_ttl", 86400)
                if datetime.now() - cached_time < timedelta(seconds=cache_ttl):
                    vulnerabilities = cache_data.get("vulnerabilities", [])
                    # Update memory cache
                    self._memory_cache[cache_key] = vulnerabilities
                    return vulnerabilities
            except (json.JSONDecodeError, ValueError, OSError) as e:
                logger.debug(f"Cache read error: {str(e)}")

        return None

    def _save_to_cache(
        self, cache_key: str, vulnerabilities: List[Dict[str, Any]]
    ) -> None:
        """
        Save results to cache.

        Args:
            cache_key: Cache key
            vulnerabilities: Vulnerabilities to cache
        """
        # Save to memory cache
        self._memory_cache[cache_key] = vulnerabilities

        # Save to disk cache
        cache_file = self._cache_dir / f"{cache_key}.json"
        try:
            cache_data = {
                "timestamp": datetime.now().isoformat(),
                "vulnerabilities": vulnerabilities,
            }
            with open(cache_file, "w") as f:
                json.dump(cache_data, f, indent=2)
        except OSError as e:
            logger.warning(f"Failed to write cache: {str(e)}")

    def _check_rate_limit(self) -> None:
        """
        Check and enforce rate limiting.

        Raises:
            RateLimitError: If rate limit would be exceeded
        """
        now = time.time()
        # Remove requests older than 60 seconds
        self._request_times = [t for t in self._request_times if now - t < 60]

        if len(self._request_times) >= self.requests_per_minute:
            # Calculate wait time
            oldest = self._request_times[0]
            wait_time = 60 - (now - oldest)
            logger.warning(f"Rate limit reached, waiting {wait_time:.1f} seconds")
            time.sleep(wait_time + 0.1)
            # Retry check after waiting
            self._check_rate_limit()
        else:
            self._request_times.append(now)

    def cache_results(self, results: Dict[str, Any]) -> None:
        """
        Manually cache vulnerability scan results.

        Args:
            results: Results dictionary to cache
        """
        package = results.get("package")
        version = results.get("version")
        ecosystem = results.get("ecosystem")
        vulnerabilities = results.get("vulnerabilities", [])

        if not all([package, version, ecosystem]):
            logger.warning("Incomplete results, skipping cache")
            return

        # Type narrowing
        assert isinstance(package, str)
        assert isinstance(version, str)
        assert isinstance(ecosystem, str)

        cache_key = self._get_cache_key(package, version, ecosystem)
        self._save_to_cache(cache_key, vulnerabilities)
        logger.info(f"Cached results for {package}@{version}")

    def clear_cache(self) -> None:
        """Clear all cached data."""
        self._memory_cache.clear()

        # Clear disk cache
        for cache_file in self._cache_dir.glob("*.json"):
            try:
                cache_file.unlink()
            except OSError as e:
                logger.warning(f"Failed to delete cache file: {str(e)}")

        logger.info("Cache cleared")

    def get_statistics(self) -> Dict[str, Any]:
        """
        Get matcher statistics.

        Returns:
            Dictionary with cache and query statistics
        """
        cache_files = list(self._cache_dir.glob("*.json"))

        return {
            "memory_cache_size": len(self._memory_cache),
            "disk_cache_files": len(cache_files),
            "enabled_databases": self.databases,
            "requests_in_last_minute": len(self._request_times),
            "rate_limit": self.requests_per_minute,
        }

    def __enter__(self):
        """Context manager entry."""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        self.session.close()
